# -*- coding: utf-8 -*-
"""Protasov_L7_unfinished_(1) (11).ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1anme7rr_3zZn5Sp_ZDPj0HCCmXKTlsUG
"""

#pip install --upgrade xlrd

# Commented out IPython magic to ensure Python compatibility.
#%pip install statsmodels
import numpy as np
import matplotlib.pyplot as plt 
import matplotlib.dates as mpl_dates

import pandas as pd 
import seaborn as sns
from datetime import datetime

import warnings
warnings.filterwarnings('ignore')



# %matplotlib inline


from sklearn.preprocessing import StandardScaler

from sklearn.model_selection import train_test_split

def read_signals(filename):
    samples_count = 5000

    c = ['name', 'x', 'y']
    for i in range(0, samples_count):
        c.append(f'v{i}')
    c = c + ['cluster', 'p0', 'p1', 'p2', 'p3']

    df = pd.read_csv(filename, names=c, dtype=np.float32)
    df = df.set_index('name', drop=True)

    return df


def write_signals(df, filename):
    df.to_csv(filename, header=False)


#if __name__ == "__main__":
    # Example
    #df = read_signals('signals.csv')
    #print(df)
    #write_signals(df, 'signals-out.csv')

"""Считывание и переименовывание данных"""

indata = read_signals('signals.csv')

dropcol = ['x','y','cluster', 'p0', 'p1', 'p2', 'p3']
hp0 = []
hp1 = []
hp2 = []
hp3 = []
#x = np.arange(5000)
x = []
y = []
for index, row in indata.iterrows():
    if row['p0'] != -1:
      hp0.append(row['v{}'.format(int(row['p0']))])        
    else:
      hp0.append(-1)
    if row['p1'] != -1:
      hp1.append(row['v{}'.format(int(row['p1']))])
    else:
      hp1.append(-1)
    if row['p2'] != -1:
      hp2.append(row['v{}'.format(int(row['p2']))])
    else:
      hp2.append(-1)
    if row['p3'] != -1:
      hp3.append(row['v{}'.format(int(row['p3']))])
    else:
      hp3.append(-1)
b = False
for index, row in indata.iterrows():
  if row['p1'] != -1:
     marker = row['p1']
     b = True
     k = 0
     for i in range(0, 5000):
       k += 1
       if k == 25:
         y.append(row['v{}'.format(i)])
         x.append(i)
         k = 0 
  if b: break
          
indata['vp0'] = hp0
indata['vp1'] = hp1
indata['vp2'] = hp2
indata['vp3'] = hp3

print(indata.head(2))

traindata = indata.loc[indata['cluster'] != -1]
testdata = indata.loc[indata['cluster'] == -1]
p0data = indata.loc[indata['p0'] != -1]
#p0data = p0data.loc[p0data['p0'] != 0]
p1data = indata.loc[indata['p1'] != -1]
p2data = indata.loc[indata['p2'] != -1]
p3data = indata.loc[indata['p3'] != -1]
vp0data = indata.loc[indata['vp0'] != -1]
vp1data = indata.loc[indata['vp1'] != -1]
vp2data = indata.loc[indata['vp2'] != -1]
vp3data = indata.loc[indata['vp3'] != -1]
print(p1data)

from sklearn.metrics import confusion_matrix
from sklearn.model_selection import train_test_split
from sklearn.tree import DecisionTreeClassifier
from sklearn.metrics import accuracy_score
from sklearn.metrics import classification_report

from sklearn import preprocessing
from sklearn import utils
from sklearn.ensemble import RandomForestClassifier
from sklearn.ensemble import RandomForestRegressor

def getprecision(datar, key):
  dropcol = ['x','y','cluster', 'p0', 'p1', 'p2', 'p3', 'vp0', 'vp1', 'vp2', 'vp3']
  X = datar.drop(dropcol, axis=1)
  Y = datar[key].values
  scaler = StandardScaler()
  scaler.fit(X)
  #X = scaler.transform(X)
  X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size = 0.1, random_state=42)
  print(X_train.shape)
  print(y_train.shape)
  print(X_test.shape)
  print(y_test.shape)
  print(y_test)



  from sklearn.model_selection import RandomizedSearchCV
  # Number of trees in random forest
  n_estimators = [int(x) for x in np.linspace(start = 200, stop = 2000, num = 10)]
  # Number of features to consider at every split
  max_features = ['auto', 'sqrt']
  # Maximum number of levels in tree
  max_depth = [int(x) for x in np.linspace(10, 110, num = 11)]
  max_depth.append(None)
  # Minimum number of samples required to split a node
  min_samples_split = [2, 5, 10]
  # Minimum number of samples required at each leaf node
  min_samples_leaf = [1, 2, 4]
  # Method of selecting samples for training each tree
  bootstrap = [True, False]
  # Create the random grid
  random_grid = {'n_estimators': n_estimators,
               'max_features': max_features,
               'max_depth': max_depth,
               'min_samples_split': min_samples_split,
               'min_samples_leaf': min_samples_leaf,
               'bootstrap': bootstrap}
  #print(random_grid)

  # Use the random grid to search for best hyperparameters
  # First create the base model to tune
  rf = RandomForestClassifier()
  # Random search of parameters, using 3 fold cross validation, 
  # search across 100 different combinations, and use all available cores
  rf_random = RandomizedSearchCV(estimator = rf, param_distributions = random_grid, n_iter = 20, cv = 3, verbose=2, random_state=42, n_jobs = -1)
  # Fit the random search model
  #rf_random.fit(X_train, y_train)
  #best_random = rf_random.best_estimator_
  #rf_random.best_params_
  #print(best_random)
  #print(rf_random.best_params_)

  settings = {'n_estimators': 600, 'min_samples_split': 10, 'min_samples_leaf': 4, 'max_features': 'sqrt', 'max_depth': 90, 'bootstrap': False}







  clf = RandomForestClassifier(bootstrap=False, max_depth=90, max_features='sqrt', min_samples_leaf=4, min_samples_split=10, n_estimators=600)
  clf.fit(X_train, y_train)
  

         

  y_pred = clf.predict(X_test)
  print("____________ForestC __________")
  print("Test:")
  print(y_test)
  print("Predicted values:")
  print(y_pred)
  errors = abs(y_pred - y_test)
  mape = 100 * np.mean(errors / y_test)
  accuracy = 100 - mape
  print('Model Performance')
  print('Average Error: {:0.4f} degrees.'.format(np.mean(errors)))
  print('Accuracy = {:0.2f}%.'.format(accuracy))

  return clf    

dropcol = ['x','y','cluster', 'p0', 'p1', 'p2', 'p3', 'vp0', 'vp1', 'vp2', 'vp3']

model = getprecision(p0data, 'p0')
indata['pred0'] = model.predict(indata.drop(dropcol, axis=1))
dropcol.append('pred0')

model = getprecision(p1data, 'p1')
indata['pred1'] = model.predict(indata.drop(dropcol, axis=1))
dropcol.append('pred1')

model = getprecision(p2data, 'p2')
indata['pred2'] = model.predict(indata.drop(dropcol, axis=1))
dropcol.append('pred2')

model = getprecision(p3data, 'p3')
indata['pred3'] = model.predict(indata.drop(dropcol, axis=1))
dropcol.append('pred3')

indata = indata.drop(['vp0', 'vp1', 'vp2', 'vp3'], axis=1)

indata.head(15)

indata.tail(15)

hp0 = []
hp1 = []
hp2 = []
hp3 = []

for index, row in indata.iterrows():
    if row['pred0'] != -1:
      hp0.append(row['v{}'.format(int(row['pred0']))])        
    else:
      hp0.append(-1)
    if row['pred1'] != -1:
      hp1.append(row['v{}'.format(int(row['pred1']))])
    else:
      hp1.append(-1)
    if row['pred2'] != -1:
      hp2.append(row['v{}'.format(int(row['pred2']))])
    else:
      hp2.append(-1)
    if row['pred3'] != -1:
      hp3.append(row['v{}'.format(int(row['pred3']))])
    else:
      hp3.append(-1)

new = indata[['x', 'y', 'pred0', 'pred1', 'pred2', 'pred3', 'cluster']].copy()

c=[]
for i in range(0, 5000):
        c.append(f'v{i}')

new['min'] = indata[c].min(axis=1)
new['max'] = indata[c].max(axis=1)
new['mean'] = indata[c].mean(axis=1)
new['std'] = indata[c].std(axis=1)
new['pv0'] = hp0
new['pv1'] = hp1
new['pv2'] = hp2
new['pv3'] = hp3


traindata = new.loc[indata['cluster'] != -1]
testdata = new.loc[indata['cluster'] == -1]

traindata = traindata.reindex(columns=['x',	'y', 'pred0',	'pred1',	'pred2',	'pred3', 'min',	'max',	'mean',	'std',	'pv0',	'pv1',	'pv2',	'pv3', 'cluster'])
traindata.head(5)

mask = np.zeros_like(traindata.corr())
triangle_indices=np.triu_indices_from(mask)
mask[triangle_indices]= True
from numpy.ma.core import mask_or
plt.figure(figsize=(10,6))
sns.heatmap(traindata.corr(), annot=True, annot_kws={'size':10}, mask=mask)
sns.set_style('white')
plt.xticks(fontsize=10)
plt.yticks(fontsize=10)

dropcol = ['x',	'y',	'pred0',	'pred1',	'pred2',	'pred3',	'max', 'cluster']
X = traindata.drop(dropcol, axis=1)
Y = traindata['cluster'].values
print(X)
print(Y)

X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size = 0.2, random_state=2)
print(X_train.shape)
print(y_train.shape)
print(X_test.shape)
print(y_test.shape)
print(y_test)

clf1 = RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=4, n_estimators=2000)
clf1.fit(X_train, y_train)




         

y_pred = clf1.predict(X_test)
print("____________ForestC __________")
print("Test:")
print(y_test)
print("Predicted values:")
print(y_pred)
errors = abs(y_pred - y_test)
mape = 100 * np.mean(errors / y_test)
accuracy = 100 - mape
print(confusion_matrix(y_test, y_pred))
print(classification_report(y_test, y_pred))
print('Model Performance')
print('Average Error: {:0.4f} degrees.'.format(np.mean(errors)))
print('Accuracy = {:0.2f}%.'.format(accuracy))

new['pcluster'] = clf1.predict(new.drop(dropcol, axis=1))
print(new.head())

dropcol = ['pred0',	'pred1',	'pred2',	'pred3']
outdata = indata.copy()

outdata['p0'] = outdata['pred0']
outdata['p1'] = outdata['pred1']
outdata['p2'] = outdata['pred2']
outdata['p3'] = outdata['pred3']
outdata['cluster'] = new['pcluster']
outdata = outdata.drop(dropcol, axis=1)

print(outdata.head())
write_signals(outdata, 'signals-out.csv')